## Design

Fast-ai "tokenizer contract"

> what any custom tokenizer ( our BytePairTokenizer) include ) must satisfy to plug into the mid-level API without surprises.

| Aspect                                     | Concrete requirement                                                                                                                                                                                                     | Where we saw it                                                     |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------- |
| **Callable signature**                     | `tok(texts_iterable) → iter_of_token_lists`<br>- `texts_iterable` can be any Python iterator of **raw strings**.<br>- Return value must itself be an **iterator** that yields one `list[str]` (tokens) per input string. | `tok = WordTokenizer(); list(tok(texts))` ran fine under `%timeit`. |
| **Lazy, one-pass**                         | The tokenizer **must not consume the iterator ahead of time** or expect a list—fastai streams large corpora (e.g. IMDb 25 k docs) without loading everything into RAM.                                                   | We fed a generator: `texts = (t for t in df['text'])`.              |
| **Token type**                             | Each token **must be a Python `str`**, not bytes or ints, because downstream `Numericalize` maps *str tokens* → int IDs.                                                                                                 | Default `WordTokenizer` returns `['This', 'is', 'great', '!']`.     |
| **Order preserved**                        | Output iterator order must match input order; fastai aligns labels with texts by position.                                                                                                                               | `list(tok(texts))` kept IMDb rows in place.                         |
| **Stateless / re-entrant**                 | Calling `tok(texts2)` after `tok(texts1)` should behave independently (no hidden cursor, no global cache).                                                                                                               | We could reuse the same `tok` object multiple times.                |
| **Pickle-able**                            | Anything you plan to stick inside a `DataBlock` must survive `dill` pickling (used in fastai’s parallel dataloaders). Avoid non-picklable Cython / lambda attributes unless you wrap them carefully.                     | WordTokenizer’s regex pre-compiles but is pickle-safe.              |
| **Instantiation cost ≪ tokenisation cost** | The object is often re-created in each worker process; heavyweight training state belongs elsewhere.                                                                                                                     | `WordTokenizer()` is cheap (regex compile).                         |
| **Optional niceties**                      | • `decode(ids)` helper (nice for demos).<br>• `special_toks` list to register PAD/BOS/EOS.<br>• `__repr__` for notebook friendliness.                                                                                    | Not required by the core pipeline but good practice.                |
